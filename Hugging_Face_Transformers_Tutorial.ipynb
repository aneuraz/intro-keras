{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aneuraz/intro-keras/blob/master/Hugging_Face_Transformers_Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKB26q-4-X8r"
      },
      "source": [
        "# Tutoriel Hugging Face Transformers\n",
        "\n",
        "> adapté de CS224N: Hugging Face Transformers Tutorial (Winter '22), Ben Newman ([version CS224N](https://colab.research.google.com/drive/1pxc-ehTtnVM72-NViET_D2ZqOlpOi2LH?usp=sharing))\n",
        "\n",
        "Ce notebook vous donnera une introduction à la bibliothèque Python Hugging Face Transformers et à quelques modèles courants que vous pouvez utiliser pour en tirer parti. Cette librairie particulièrement utile pour utiliser ou fine-tuner des modèles transformers préentrainés dans vos projets.\n",
        "\n",
        "Hugging Face fournit un accès aux modèles (à la fois le code qui les met en œuvre et leurs poids pré-entraînés), aux tokeniseurs spécifiques aux modèles, ainsi qu'aux pipelines pour les tâches NLP courantes et aux datasets et aux métriques. Il a des implémentations en PyTorch, Tensorflow et Flax (même si nous utiliserons ici les versions PyTorch !)\n",
        "\n",
        "Nous allons passer en revue quelques cas d'utilisation :\n",
        "\n",
        "- Aperçu des tokeniseurs et des modèles\n",
        "- Finetuning - pour votre propre tâche. Nous utiliserons un exemple de classification de sentiments.\n",
        "\n",
        "Nous pouvons distinguer différents types de projets de NLP:\n",
        "1. Appliquer un modèle pré-entrainé à une nouvelle tâche ou un nouveau domaine et explorer des solutions pour résoudre cette question\n",
        "2. Implémenter une nouvelle architecture de réseaux de neurones et démontrer ses performances sur des données.\n",
        "3. Analyser le comportement d'un modèle: comment il représente les connaissances linguistiques ou quel type de phénomènes il peut gérer ou les erreurs qu'il commet. \n",
        "\n",
        "Parmi ces types de projets, `transformers` pourra être plus utile pour 1. et 3. (on peut aussi l'utiliser pour définir de nouvelles architectures mais c'est plus compliqué et nous n'aborderons pas cet aspect).\n",
        "\n",
        "Voici quelques ressources complémentaires (en anglais) concernant la librairie `transformers`:\n",
        "\n",
        "* [Hugging Face Docs](https://huggingface.co/docs/transformers/index)\n",
        "  * documentation claire\n",
        "  * Tutorials, pas-à-pas, et notebooks d'exemples\n",
        "  * Liste des modèles disponibles\n",
        "* [Cours Hugging Face](https://huggingface.co/course/)\n",
        "\n"
      ],
      "id": "sKB26q-4-X8r"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "9EhWoZef-X8u"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet transformers datasets sentencepiece \n"
      ],
      "id": "9EhWoZef-X8u"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q9th7mpc-X8v"
      },
      "outputs": [],
      "source": [
        "#from collections import defaultdict, Counter\n",
        "#import json\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def print_encoding(model_inputs, indent=4):\n",
        "    indent_str = \" \" * indent\n",
        "    print(\"{\")\n",
        "    for k, v in model_inputs.items():\n",
        "        print(indent_str + k + \":\")\n",
        "        print(indent_str + indent_str + str(v))\n",
        "    print(\"}\")"
      ],
      "id": "Q9th7mpc-X8v"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmXezUMg2idv"
      },
      "source": [
        "## Partie 0: Patterns courants pour l'utilisation de Hugging Face Transformers\n",
        "\n",
        "Commençons par une utilisation commune de Transformers: l'analyse de sentiments. \n",
        "\n",
        "Premièrement, trouver un modèle sur [le hub](https://huggingface.co/models). Tout le monde peut uploader son modèle pour qu'il puisse être utilisé par tous. (Nous utiliserons le modèle d'analyse de sentiment issu de [ce repo](https://huggingface.co/citizenlab/twitter-xlm-roberta-base-sentiment-finetunned)).\n",
        "\n",
        "Ensuite, deux objets différents doivent être initialisés: \n",
        "- un **tokenizer**: converti les textes en listes d'ids de vocabulaire (input ids) que le modèle nécessite\n",
        "- un **modèle**: prend les ids de vocabulaire et produit une prédiction\n"
      ],
      "id": "qmXezUMg2idv"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySLmJ0Z-oD35"
      },
      "source": [
        "![full_nlp_pipeline.png](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/full_nlp_pipeline.svg)\n",
        "From [https://huggingface.co/course/chapter2/2?fw=pt](https://huggingface.co/course/chapter2/2?fw=pt)"
      ],
      "id": "ySLmJ0Z-oD35"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mcsii_O42Z8Q"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "model_name = \"cmarkea/distilcamembert-base-sentiment\"\n",
        "\n",
        "# Initialise le tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "# Initialise le  model\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)"
      ],
      "id": "Mcsii_O42Z8Q"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kT_zeWRBoD36"
      },
      "outputs": [],
      "source": [
        "inputs = \"Je suis vraiment content de faire ce fabuleux tutoriel\"\n",
        "tokenized_inputs = tokenizer(inputs, return_tensors=\"pt\")\n",
        "outputs = model(**tokenized_inputs)\n",
        "print(outputs)\n",
        "labels = ['1 star','2 stars','3 stars','4 stars','5 stars']\n",
        "prediction = torch.argmax(outputs.logits)\n",
        "\n",
        "\n",
        "print(\"Input:\")\n",
        "print(inputs)\n",
        "print()\n",
        "print(\"Tokenized Inputs:\")\n",
        "print_encoding(tokenized_inputs)\n",
        "print()\n",
        "print(\"Model Outputs:\")\n",
        "print(outputs)\n",
        "print()\n",
        "print(f\"La prédiction est {labels[prediction]}\")"
      ],
      "id": "kT_zeWRBoD36"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7jvH9haoD37"
      },
      "source": [
        "### 0.1 Tokenizers"
      ],
      "id": "a7jvH9haoD37"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43FLbwgz-X83"
      },
      "source": [
        "Les modèles pré-entraînés sont mis en œuvre avec des **tokenizers** qui sont utilisés pour prétraiter leurs entrées. Les tokenizers prennent des chaînes brutes ou des listes de chaînes en entrée et produisent ce qui revient à des dictionnaires contenant les entrées du modèle.\n",
        "\n",
        "Vous pouvez accéder aux tokenizers soit en utilisant la classe `Tokenize`\n",
        " spécifique au modèle que vous souhaitez utiliser (ici `DistilBert`), soit en utilisant la classe `AutoTokenizer`. Les tokenizers rapides sont écrits en Rust, tandis que leurs versions lentes sont écrites en Python."
      ],
      "id": "43FLbwgz-X83"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pu6L0lWG-X83",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "from transformers import CamembertTokenizer, CamembertTokenizerFast, AutoTokenizer\n",
        "\n",
        "base_model = \"cmarkea/distilcamembert-base\"\n",
        "\n",
        "tokenizer = CamembertTokenizer.from_pretrained(base_model)      # written in Python\n",
        "print(tokenizer)\n",
        "tokenizer = CamembertTokenizerFast.from_pretrained(base_model)  # written in Rust\n",
        "print(tokenizer)\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model) # convenient! Defaults to Fast\n",
        "print(tokenizer)"
      ],
      "id": "Pu6L0lWG-X83"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zrPzbBhR-X84"
      },
      "outputs": [],
      "source": [
        "# Voici comment on appelle le tokinzer\n",
        "input_str = \"J'aime la glace à la vanille\"\n",
        "tokenized_inputs = tokenizer(input_str)\n",
        "\n",
        "\n",
        "print(\"Vanilla Tokenization\")\n",
        "print_encoding(tokenized_inputs)\n",
        "print()\n",
        "\n",
        "# 2 méthodes pour récupérer les input_ids:\n",
        "print(tokenized_inputs.input_ids)\n",
        "print(tokenized_inputs[\"input_ids\"])"
      ],
      "id": "zrPzbBhR-X84"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E_8C6L2G-X85"
      },
      "outputs": [],
      "source": [
        "cls = [tokenizer.cls_token_id]\n",
        "sep = [tokenizer.sep_token_id]\n",
        "\n",
        "# La Tokenization se décompose en 3 étapes: \n",
        "input_tokens = tokenizer.tokenize(input_str)     \n",
        "input_ids = tokenizer.convert_tokens_to_ids(input_tokens)\n",
        "input_ids_special_tokens = cls + input_ids + sep\n",
        "\n",
        "decoded_str = tokenizer.decode(input_ids_special_tokens)\n",
        "\n",
        "print(\"text de départ:                \", input_str)\n",
        "print(\"tokenize:             \", input_tokens)\n",
        "print(\"convert_tokens_to_ids:\", input_ids)\n",
        "print(\"add special tokens:   \", input_ids_special_tokens)\n",
        "print(\"--------\")\n",
        "print(\"decode:               \", decoded_str)\n",
        "\n",
        "# NOTE ces étapes ne créent par le masque d'attention (attention mask)"
      ],
      "id": "E_8C6L2G-X85"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tdoZ3EEU-X86"
      },
      "outputs": [],
      "source": [
        "# Pour les Fast Tokenizers,il y a une autre option également: \n",
        "inputs = tokenizer._tokenizer.encode(input_str)\n",
        "\n",
        "print(input_str)\n",
        "print(\"-\"*5)\n",
        "print(f\"Number of tokens: {len(inputs)}\")\n",
        "print(f\"Ids: {inputs.ids}\")\n",
        "print(f\"Tokens: {inputs.tokens}\")\n",
        "print(f\"Special tokens mask: {inputs.special_tokens_mask}\")\n",
        "print()\n",
        "print(\"char_to_word retourne le wordpiece d'un caractère de l'input\")\n",
        "char_idx = 11\n",
        "print(f\"Par exemple, le {char_idx + 1}ème caractère de l'input est '{input_str[char_idx]}',\"+\\\n",
        "      f\" et il fait parti du wordpiece {inputs.char_to_token(char_idx)}, '{inputs.tokens[inputs.char_to_token(char_idx)]}'\")"
      ],
      "id": "tdoZ3EEU-X86"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vt5WV-6S-X87"
      },
      "outputs": [],
      "source": [
        "# Autres propriétés intéressantes:\n",
        "# Le tokenizer peut renvoyer directement des tensors pytorch\n",
        "model_inputs = tokenizer(\"J'aime la glace à la vanille\", return_tensors=\"pt\")\n",
        "print(\"PyTorch Tensors:\")\n",
        "print_encoding(model_inputs)"
      ],
      "id": "vt5WV-6S-X87"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HI3bAzpeoD3_"
      },
      "outputs": [],
      "source": [
        "# On peut passer plusieurs strings à la fois au tokenizer et effectuer un padding en même temps:\n",
        "model_inputs = tokenizer([\"J'aime la glace à la vanille\",\n",
        "                         \"Pour le Parlement européen, l'intelligence artificielle \" +\\\n",
        "                         \"représente tout outil utilisé par une machine afin de\" +\\\n",
        "                         \"« reproduire des comportements liés aux humains. \"\n",
        "                         ],\n",
        "                         return_tensors=\"pt\",\n",
        "                         padding=True,\n",
        "                         truncation=True)\n",
        "print(f\"Pad token: {tokenizer.pad_token} | Pad token id: {tokenizer.pad_token_id}\")\n",
        "print(\"Padding:\")\n",
        "print_encoding(model_inputs)"
      ],
      "id": "HI3bAzpeoD3_"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "iSZat-nkoD3_"
      },
      "outputs": [],
      "source": [
        "# Il est également possible de décoder un batch entier:\n",
        "print(\"Batch Decode:\")\n",
        "print(tokenizer.batch_decode(model_inputs.input_ids))\n",
        "print()\n",
        "print(\"Batch Decode: (no special characters)\")\n",
        "print(tokenizer.batch_decode(model_inputs.input_ids, skip_special_tokens=True))"
      ],
      "id": "iSZat-nkoD3_"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmZNLz_noD4A"
      },
      "source": [
        "Pour plus d'informations, vous pouvez aller voir sur: \n",
        "[la documentation Hugging Face Transformers](https://huggingface.co/docs/transformers/main_classes/tokenizer) et la [librairie Hugging Face Tokenizers](https://huggingface.co/docs/tokenizers/python/latest/quicktour.html) (pour les Fast Tokenizers). La librairie Tokenizers vous permet également d'entrainer vos propres tokenizers!"
      ],
      "id": "JmZNLz_noD4A"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6juLjnNt-X87"
      },
      "source": [
        "### 0.2 Modèles\n",
        "\n",
        "Initialiser un modèle est très similaire à l'initialisation d'un tokenizer. Vous pouvez utiliser une classe spécifique au modèle ou passer par la classe AutoModel. Il peut être utile de privilégier AutoModel car si l'on veut comparer des modèles de classes différentes, cela évite d'alourdir le code et il suffit de changer le nom du modèle dans le code. \n",
        "\n",
        "La plupart des modèles transformers préentrainés ont une architecture similaire, il faut ajouter une tête (head) à entrainer en fonction de la tâche à effectuer: classification de séquence, question answering, classification de tokens, ... \n",
        "Par exemple, nous voulons faire de l'analyse de sentiment. Il s'agit d'une tâche de classification de séquence, nous avons donc besoin de la classe `XLMRobertaForSequenceClassification`. Si nous voulions continuer d'entrainer XLMRoberta sur sa tâche de masked-language modelling, nous pourrions utiliser `XLMRobertaForMaskedLM`, et si nous voulions uniquement récupérer la représentation issue du modèle nous pourrions utiliser `XLMRobertaModel`. \n",
        "\n",
        "\n",
        "Voici une illustration de l'architecture des modèles dans HF Transformers: \n",
        "![model_illustration.png](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/transformer_and_head.svg)\n",
        "source: [https://huggingface.co/course/chapter2/2?fw=pt](https://huggingface.co/course/chapter2/2?fw=pt).\n",
        "\n",
        "\n",
        "Voici quelques examples de têtes spécialisées.\n",
        "```\n",
        "*\n",
        "*ForMaskedLM\n",
        "*ForSequenceClassification\n",
        "*ForTokenClassification\n",
        "*ForQuestionAnswering\n",
        "*ForMultipleChoice\n",
        "...\n",
        "```\n",
        "où `*` peut être `AutoModel` ou un modèle spécifique (e.g. `DistilBert`)\n",
        "\n",
        "Il existe 3 types de modèles: \n",
        "* Encodeurs (e.g. BERT)\n",
        "* Decodeurs (e.g. GPT2)\n",
        "* Modèles Encodeur-Decodeur (e.g. BART or T5)\n",
        "\n",
        "En fonction du type de modèle, les tâches supportées et donc les classes associées peuvent changer.\n",
        "\n",
        "La liste complète des choix possibles est disponible dans la [documentation] (https://huggingface.co/docs/transformers/model_doc/auto). Attention, tous les modèles ne ne sont pas compatibles avec toutes les architectures. Par exemple, DistilBERT n'est pas compatible avec les modèles Seq2seq car c'est un modèle constitué uniquement d'un encodeur. \n"
      ],
      "id": "6juLjnNt-X87"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RXm1K2sF-X88",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(base_model, num_labels=5)\n"
      ],
      "id": "RXm1K2sF-X88"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1opFV7Vi-X88"
      },
      "source": [
        "Un Warning apparait car les paramètres de la tête de classification de séquence n'ont pas encore été entrainés.\n",
        "\n",
        "Passer les inputs au modèle est très simple. Les modèles prennent les inputs en \n",
        "keyword argument."
      ],
      "id": "1opFV7Vi-X88"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TDZ72k-U-X89"
      },
      "outputs": [],
      "source": [
        "model_inputs = tokenizer(input_str, return_tensors=\"pt\")\n",
        "\n",
        "# Option 1\n",
        "model_outputs = model(input_ids=model_inputs.input_ids, \n",
        "                      attention_mask=model_inputs.attention_mask)\n",
        "\n",
        "# Option 2 - Les clés du dictionnaire sont les mêmes que les arguments attendus par le modèle\n",
        "\n",
        "# f({k1: v1, k2: v2}) = f(k1=v1, k2=v2)\n",
        "\n",
        "model_outputs = model(**model_inputs)\n",
        "\n",
        "print(model_inputs)\n",
        "print()\n",
        "print(model_outputs)\n",
        "print()\n",
        "print(f\"Distribution over labels: {torch.softmax(model_outputs.logits, dim=1)}\")"
      ],
      "id": "TDZ72k-U-X89"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvie4gYD-X8-"
      },
      "source": [
        "These models are just Pytorch Modules! You can can calculate the loss with your `loss_func` and call `loss.backward`. You can use any of the optimizers or learning rate schedulers that you used"
      ],
      "id": "rvie4gYD-X8-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Irxo7sDboD4C"
      },
      "outputs": [],
      "source": [
        "# You can calculate the loss like normal\n",
        "label = torch.tensor([1])\n",
        "loss = torch.nn.functional.cross_entropy(model_outputs.logits, label)\n",
        "print(loss)\n",
        "loss.backward()\n",
        "\n",
        "# You can get the parameters\n",
        "list(model.named_parameters())[0]"
      ],
      "id": "Irxo7sDboD4C"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpHeG1zDoD4D"
      },
      "source": [
        "Hugging Face provides an additional easy way to calculate the loss as well:"
      ],
      "id": "mpHeG1zDoD4D"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S148gCyG-X8-"
      },
      "outputs": [],
      "source": [
        "# To calculate the loss, we need to pass in a label:\n",
        "model_inputs = tokenizer(input_str, return_tensors=\"pt\")\n",
        "\n",
        "model_inputs['labels'] = torch.tensor([1])\n",
        "\n",
        "model_outputs = model(**model_inputs)\n",
        "\n",
        "\n",
        "print(model_outputs)\n",
        "print()\n",
        "print(f\"Model predictions: {labels[model_outputs.logits.argmax()]}\")"
      ],
      "id": "S148gCyG-X8-"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Y6E3IxzoD4E"
      },
      "source": [
        "One final note - you can get the hidden states and attention weights from the models really easily. This is particularly helpful if you're working on an analysis project. (For example, see [What does BERT look at?](https://arxiv.org/abs/1906.04341))."
      ],
      "id": "7Y6E3IxzoD4E"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5WzqhpquoD4E"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModel\n",
        "\n",
        "model = AutoModel.from_pretrained(base_model, output_attentions=True, output_hidden_states=True)\n",
        "model.eval()\n",
        "\n",
        "model_inputs = tokenizer(input_str, return_tensors=\"pt\")\n",
        "with torch.no_grad():\n",
        "    model_output = model(**model_inputs)\n",
        "\n",
        "\n",
        "print(\"Hidden state size (per layer):  \", model_output.hidden_states[0].shape)\n",
        "print(\"Attention head size (per layer):\", model_output.attentions[0].shape)     # (layer, batch, query_word_idx, key_word_idxs)\n",
        "                                                                               # y-axis is query, x-axis is key\n",
        "print(model_output)    "
      ],
      "id": "5WzqhpquoD4E"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SH_MAK-soD4F"
      },
      "outputs": [],
      "source": [
        "tokens = tokenizer.convert_ids_to_tokens(model_inputs.input_ids[0])\n",
        "print(tokens)\n",
        "n_tokens = len(tokens)\n",
        "\n",
        "n_layers = len(model_output.attentions)\n",
        "n_heads = len(model_output.attentions[0][0])\n",
        "fig, axes = plt.subplots(n_layers, n_heads)\n",
        "fig.set_size_inches(18.5*2, 10.5*2)\n",
        "for layer in range(n_layers):\n",
        "    for i in range(n_heads):\n",
        "        axes[layer, i].imshow(model_output.attentions[layer][0, i])\n",
        "        axes[layer][i].set_xticks(list(range(n_tokens)))\n",
        "        axes[layer][i].set_xticklabels(labels=tokens, rotation=\"vertical\")\n",
        "        axes[layer][i].set_yticks(list(range(n_tokens)))\n",
        "        axes[layer][i].set_yticklabels(labels=tokens)\n",
        "\n",
        "        if layer == n_layers:\n",
        "            axes[layer, i].set(xlabel=f\"head={i}\")\n",
        "        if i == 0:\n",
        "            axes[layer, i].set(ylabel=f\"layer={layer}\")\n",
        "            \n",
        "plt.subplots_adjust(wspace=0.1)\n",
        "plt.show()"
      ],
      "id": "SH_MAK-soD4F"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uumcErs2-X80"
      },
      "source": [
        "## Part 1: Finetuning\n",
        "\n",
        "Vous allez surement devoir finetuner un modèle préentrainé pour vos projets.\n",
        "\n"
      ],
      "id": "uumcErs2-X80"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDdGp4Ua-X81"
      },
      "source": [
        "\n",
        "### 2.1 Charger un dataset\n",
        "\n",
        "En plus des modèles, on peut trouver des datasets sur [le hub](https://huggingface.co/datasets) Hugging Face.\n"
      ],
      "id": "WDdGp4Ua-X81"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OTsW-Wwi-X81"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, DatasetDict\n",
        "\n",
        "amazon_dataset = load_dataset(\"amazon_reviews_multi\", \"fr\")\n",
        "\n",
        "# Prenons uniquement les 128 premiers caractères pour accélerer l'entrainement.\n",
        "def truncate(example):\n",
        "    return {\n",
        "        'review_body': \" \".join(example['review_body'].split()[:128]),\n",
        "        'label': example['stars']-1\n",
        "    }\n",
        "\n",
        "# Prend 1280 examples aléatoires pour le train et 320 pour la validation\n",
        "small_amazon_dataset = DatasetDict(\n",
        "    train=amazon_dataset['train'].shuffle(seed=1111).select(range(1280)).map(truncate),\n",
        "    val=amazon_dataset['train'].shuffle(seed=1111).select(range(1280, 1600)).map(truncate),\n",
        ")"
      ],
      "id": "OTsW-Wwi-X81"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bBS4c44A-X82"
      },
      "outputs": [],
      "source": [
        "small_amazon_dataset['train'][:10]"
      ],
      "id": "bBS4c44A-X82"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3bjqop3N-X8_"
      },
      "outputs": [],
      "source": [
        "# Prepare le dataset - tokenize le dataset en batches de 32 exemples.\n",
        "small_tokenized_dataset = small_amazon_dataset.map(\n",
        "    lambda example: tokenizer(example['review_body'], padding=True, truncation=True),\n",
        "    batched=True,\n",
        "    batch_size=32\n",
        ")\n",
        "\n",
        "small_tokenized_dataset = small_tokenized_dataset.remove_columns([\"review_body\", 'review_id', 'product_id', 'reviewer_id', 'stars','review_title', 'language', 'product_category'])\n",
        "small_tokenized_dataset = small_tokenized_dataset.rename_column(\"label\", \"labels\")\n",
        "small_tokenized_dataset.set_format(\"torch\")"
      ],
      "id": "3bjqop3N-X8_"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "450eUYlf-X8_"
      },
      "outputs": [],
      "source": [
        "small_tokenized_dataset['train'][0:2]"
      ],
      "id": "450eUYlf-X8_"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q3e7_htt-X8_"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataloader = DataLoader(small_tokenized_dataset['train'], batch_size=32)\n",
        "eval_dataloader = DataLoader(small_tokenized_dataset['val'], batch_size=32)"
      ],
      "id": "Q3e7_htt-X8_"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRm6Tw_z-X8-"
      },
      "source": [
        "### 2.2 Entrainement\n",
        "\n",
        "Pour entrainer vos modèles, vous pouvez utiliser une approche Pytorch classique puisque les modèles Hugging Face sont des modules Pytorch standards. Hugging Face propose également un Trainer qui simplifie l'entrainement.\n",
        "\n"
      ],
      "id": "qRm6Tw_z-X8-"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RULukU5eoD4M"
      },
      "source": [
        "`TrainingArguments` spécifie differents paramètres d'entrainerment comme le fréquence des évaluations, où sauver les checkpoints, ... \n",
        "De nombreux aspects peuvent être customizés, vous pouvez trouver plus de renseignements [ici](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments). \n",
        "Parmi les paramètres vous trouverez:\n",
        "* learning rate, weight decay, gradient clipping, \n",
        "* checkpointing, logging, and evaluation frequency\n",
        "* destination du log (par défaut tensorboard, mais vous pouvez aussi utiliser WandB ou MLFlow )\n",
        "\n",
        "Le `Trainer` effectue l'entrainement. Vous pouvez lui passer les `TrainingArguments`, un modèle, les datasets, le tokenizer, l'optimizer, et même des model checkpoints pour reprendre un entrainement. La fonction `compute_metrics` est appelée à la fin pour calculer la métrique d'évaluation."
      ],
      "id": "RULukU5eoD4M"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FEmERCu_-X9B",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, Trainer, AutoModelForSequenceClassification\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(base_model, num_labels=5)\n",
        "\n",
        "arguments = TrainingArguments(\n",
        "    output_dir=\"sample_hf_trainer\",\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=32,\n",
        "    num_train_epochs=4,\n",
        "    evaluation_strategy=\"epoch\", # validation à la fin de chaque epoch\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    load_best_model_at_end=True,\n",
        "    seed=224\n",
        ")\n",
        "\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    \"\"\"Called at the end of validation. Gives accuracy\"\"\"\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    # calculates the accuracy\n",
        "    return {\"accuracy\": np.mean(predictions == labels)}\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=arguments,\n",
        "    train_dataset=small_tokenized_dataset['train'],\n",
        "    eval_dataset=small_tokenized_dataset['val'], # changer à test pour l'évaluation finale\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ],
      "id": "FEmERCu_-X9B"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbyK1W95-X9B"
      },
      "source": [
        "#### Callbacks: Logging and Early Stopping\n",
        "\n",
        "\n",
        "Hugging Face Transformers also allows you to write `Callbacks` if you want certain things to happen at different points during training (e.g. after evaluation or after an epoch has finished). For example, there is a callback for early stopping, and I usually write one for logging as well.\n",
        "\n",
        "For more information on callbacks see [here](https://huggingface.co/docs/transformers/main_classes/callback#transformers.TrainerCallback)."
      ],
      "id": "MbyK1W95-X9B"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vKO3TkAnoD4N"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainerCallback, EarlyStoppingCallback\n",
        "\n",
        "class LoggingCallback(TrainerCallback):\n",
        "    def __init__(self, log_path):\n",
        "        self.log_path = log_path\n",
        "        \n",
        "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
        "        _ = logs.pop(\"total_flos\", None)\n",
        "        if state.is_local_process_zero:\n",
        "            with open(self.log_path, \"a\") as f:\n",
        "                f.write(json.dumps(logs) + \"\\n\")\n",
        "\n",
        "\n",
        "trainer.add_callback(EarlyStoppingCallback(early_stopping_patience=1, early_stopping_threshold=0.0))\n",
        "#trainer.add_callback(LoggingCallback(\"sample_hf_trainer/log.jsonl\"))"
      ],
      "id": "vKO3TkAnoD4N"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tunwonc2-X9C",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# train the model\n",
        "trainer.train()"
      ],
      "id": "tunwonc2-X9C"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Q185j9V-X9C"
      },
      "outputs": [],
      "source": [
        "# evaluer le modèle est très simple\n",
        "\n",
        " results = trainer.evaluate()                           # retourne les métriques d'évaluation\n",
        "results = trainer.predict(small_tokenized_dataset['val']) # avec les prédictions aussi"
      ],
      "id": "_Q185j9V-X9C"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UJ0aGxeh-X9D"
      },
      "outputs": [],
      "source": [
        "results"
      ],
      "id": "UJ0aGxeh-X9D"
    },
    {
      "cell_type": "code",
      "source": [
        "finetuned_model = AutoModelForSequenceClassification.from_pretrained(\"sample_hf_trainer/checkpoint-160\")\n",
        "finetuned_tokenizer = AutoTokenizer.from_pretrained(\"sample_hf_trainer/checkpoint-160\")\n",
        "\n",
        "model_inputs = finetuned_tokenizer(\"j'ai aimé ce film\", return_tensors=\"pt\")\n",
        "prediction = torch.argmax(finetuned_model(**model_inputs).logits)\n",
        "print(labels[prediction])"
      ],
      "metadata": {
        "id": "y05uQnxmFvBd"
      },
      "id": "y05uQnxmFvBd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TextClassificationPipeline\n",
        "\n",
        "pipe = TextClassificationPipeline(model=finetuned_model, tokenizer=finetuned_tokenizer)\n",
        "pipe([\"j'ai aimé ce film\", \"c'était pas terrible quand même\"])"
      ],
      "metadata": {
        "id": "JV-1PRl6DomJ"
      },
      "id": "JV-1PRl6DomJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MllSTgehoD4O"
      },
      "source": [
        "Quelques astuces pour le finetuning\n",
        "\n",
        "**Bons hyperparameters par défaut.** Les hyperparamètres dépendent de la tâche et du dataset. Vous devrez faire une recherche des meilleurs hyperparamètres pour trouver les bons. Voici des bonnes baselines pour commencer: \n",
        "* Epochs: {2, 3, 4} (plus le dataset est grand et moins on fait d'epochss)\n",
        "* Batch size (le plus grand possible est le mieux )\n",
        "* Optimizer: AdamW\n",
        "* AdamW learning rate: {2e-5, 5e-5}\n",
        "* Learning rate scheduler: linear warm up for first {0, 100, 500} steps of training\n",
        "* weight_decay (l2 regularization): {0, 0.01, 0.1}\n",
        "\n",
        "Vous devriez monitorer la validation loss pour décider quand vous avez trouvé les bons hyperparamètres.\n"
      ],
      "id": "MllSTgehoD4O"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsWGQfrm-X9D"
      },
      "source": [
        "Le Trainer peut être largement personalisé. voir [ce lien](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.Trainer)\n"
      ],
      "id": "gsWGQfrm-X9D"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLAHLU4q9HYQ"
      },
      "source": [
        "## Appendix 1: Datasets personalisés\n",
        "\n",
        "Il existe plusieurs façons pour définier des datasets. Nous allons voir un exemple qui utilise les Pytorch Dataloaders.\n"
      ],
      "id": "QLAHLU4q9HYQ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MLqz11UioD4Q"
      },
      "outputs": [],
      "source": [
        "# Option 1: Load into Hugging Face Datasets\n",
        "\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "\n",
        "df = pd.read_json(\"https://raw.githubusercontent.com/aneuraz/casCliniques/main/casCliniques/trainset.json\")\n",
        "custom_dataset = Dataset.from_pandas(df)"
      ],
      "id": "MLqz11UioD4Q"
    },
    {
      "cell_type": "code",
      "source": [
        "custom_dataset[0]"
      ],
      "metadata": {
        "id": "pguCT47QLf7f"
      },
      "id": "pguCT47QLf7f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lRauc5JBoD4R"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import json, urllib.request\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class CCDataset(Dataset):\n",
        "    \"\"\"Tokenize data when we call __getitem__\"\"\"\n",
        "    def __init__(self, path, tokenizer):\n",
        "        with urllib.request.urlopen(path) as f:\n",
        "            reader = json.load(f)\n",
        "            self.data = [{\"source\": row['token'], \"target\": row[\"label\"]} for row in reader]\n",
        "        self.tokenizer = tokenizer\n",
        "            \n",
        "    def __getitem__(self, i):\n",
        "        inputs = self.tokenizer(self.data[i]['source'])\n",
        "        labels = self.tokenizer(self.data[i]['target'])\n",
        "        inputs['labels'] = labels.input_ids\n",
        "        return inputs\n"
      ],
      "id": "lRauc5JBoD4R"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "eRu5mFIpoD4R"
      },
      "outputs": [],
      "source": [
        "bart_tokenizer = AutoTokenizer.from_pretrained('facebook/bart-base')"
      ],
      "id": "eRu5mFIpoD4R"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I32zL1nEoD4S"
      },
      "outputs": [],
      "source": [
        "dataset = CCDataset(\"https://raw.githubusercontent.com/aneuraz/casCliniques/main/casCliniques/trainset.json\", bart_tokenizer)"
      ],
      "id": "I32zL1nEoD4S"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I50Sh862oD4T"
      },
      "outputs": [],
      "source": [
        "bart_tokenizer.prepare_seq2seq_batch(src_texts=[\"This is the first test.\", \"This is the second test.\"], tgt_texts=[\"Target 1\", \"Target 2\"], return_tensors=\"pt\")"
      ],
      "id": "I50Sh862oD4T"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w8frTRD3oD4T"
      },
      "outputs": [],
      "source": [
        "dataset[0]"
      ],
      "id": "w8frTRD3oD4T"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}